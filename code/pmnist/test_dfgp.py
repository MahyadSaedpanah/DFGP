import torch
import torch.nn as nn
import torch.optim as optim
from flatness_minima import DFGP  # ğŸ‘ˆ Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¯ Ø®ÙˆØ¯Øª Ø±Ùˆ import Ú©Ù†

# Ù…Ø¯Ù„ Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(2, 1)

    def forward(self, x):
        return self.linear(x)

# Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø¯Ù‡
x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
y = torch.tensor([[1.0], [0.0]])

# loss Ùˆ init
loss_fn = nn.MSELoss()
model = SimpleModel()
base_optimizer = optim.SGD(model.parameters(), lr=0.01)
doptimizer = DFGP(base_optimizer, model, rho=0.05, eta=0.01, epsilon_prime=0.01, lambda_val=0.1)

# Ø¯Ø§Ø¯Ù‡ Ù¾Ø±Ú†Ø¨â€ŒØ´Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø§ Ù†ÙˆÛŒØ² Ø³Ø§Ø¯Ù‡)
perturbed_x = x + 0.01 * torch.randn_like(x)
perturbed_targets = y  # Ú†ÙˆÙ† Mixup Ù†Ø¯Ø§Ø±ÛŒÙ…

# ========== Ù…Ø±Ø­Ù„Ù‡ Ø§ØµÙ„ÛŒ ==========
# Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ú†Ø§Ù¾ Ø¯Ø³ØªÛŒ
original_grads = {}
hessian_approximations = {}

# Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„: Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø§ÙˆÙ„ÛŒÙ‡
doptimizer.optimizer.zero_grad()
output = model(x)
loss = loss_fn(output, y)
loss.backward()

for name, param in model.named_parameters():
    if param.grad is not None:
        original_grads[name] = param.grad.clone().detach()

# Ù¾Ø±Ú†Ø¨ Ú©Ø±Ø¯Ù† ÙˆØ²Ù†â€ŒÙ‡Ø§
doptimizer.perturb_step()

# Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…: Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¨Ø§ ÙˆØ²Ù† Ù¾Ø±Ú†Ø¨â€ŒØ´Ø¯Ù‡
doptimizer.optimizer.zero_grad()
output2 = model(x)
loss2 = loss_fn(output2, y)
loss2.backward()

for name, param in model.named_parameters():
    if param.grad is not None:
        g = original_grads[name]
        g_perturbed = param.grad.clone().detach()
        h_approx = (g_perturbed - g) / doptimizer.epsilon_prime
        hessian_approximations[name] = h_approx

        # Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬
        print(f"\nğŸ§  Ù„Ø§ÛŒÙ‡: {name}")
        print(f"ğŸ¯ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø§ÙˆÙ„ÛŒÙ‡:\n{g}")
        print(f"ğŸ’¥ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¨Ø¹Ø¯ Ø§Ø² Ù¾Ø±Ú†Ø¨:\n{g_perturbed}")
        print(f"ğŸ“ Ù‡Ø³ÛŒÙ† ØªØ®Ù…ÛŒÙ†ÛŒ (HÂ·v):\n{h_approx}")

# Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¢Ù¾Ø¯ÛŒØª
doptimizer.unperturb_step()
doptimizer.step()
